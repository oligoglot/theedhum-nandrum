{
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "execution_count": 25,
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "\n# Feature Union with Heterogeneous Data Sources\n\n\nDatasets can often contain components of that require different feature\nextraction and processing pipelines.  This scenario might occur when:\n\n1. Your dataset consists of heterogeneous data types (e.g. raster images and\n   text captions)\n2. Your dataset is stored in a Pandas DataFrame and different columns\n   require different processing pipelines.\n\nThis example demonstrates how to use\n:class:`sklearn.feature_extraction.FeatureUnion` on a dataset containing\ndifferent types of features.  We use the 20-newsgroups dataset and compute\nstandard bag-of-words features for the subject line and body in separate\npipelines as well as ad hoc features on the body. We combine them (with\nweights) using a FeatureUnion and finally train a classifier on the combined\nset of features.\n\nThe choice of features is not particularly helpful, but serves to illustrate\nthe technique.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "execution_count": 26,
      "cell_type": "code",
      "source": [
        "# Author: Matt Terry <matt.terry@gmail.com>\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
        "\n",
        "    The data is expected to be stored in a 2D data structure, where the first\n",
        "    index is over features and the second is over samples.  i.e.\n",
        "\n",
        "    >> len(data[key]) == n_samples\n",
        "\n",
        "    Please note that this is the opposite convention to scikit-learn feature\n",
        "    matrixes (where the first index corresponds to sample).\n",
        "\n",
        "    ItemSelector only requires that the collection implement getitem\n",
        "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
        "    DataFrame, numpy record array, etc.\n",
        "\n",
        "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
        "               'b': [9, 4, 1, 4, 1, 3]}\n",
        "    >> ds = ItemSelector(key='a')\n",
        "    >> data['a'] == ds.transform(data)\n",
        "\n",
        "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
        "    list of dicts).  If your data is structured this way, consider a\n",
        "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : hashable, required\n",
        "        The key corresponding to the desired value in a mappable.\n",
        "    \"\"\"\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "\n",
        "\n",
        "class TextStats(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, posts):\n",
        "        return [{'length': len(text),\n",
        "                 'num_sentences': text.count('.')}\n",
        "                for text in posts]\n",
        "\n",
        "\n",
        "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
        "\n",
        "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
        "    `subject` and `body`.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, posts):\n",
        "        features = np.recarray(shape=(len(posts),),\n",
        "                               dtype=[('subject', object), ('body', object)])\n",
        "        for i, text in enumerate(posts):\n",
        "            headers, _, bod = text.partition('\\n\\n')\n",
        "            bod = strip_newsgroup_footer(bod)\n",
        "            bod = strip_newsgroup_quoting(bod)\n",
        "            features['body'][i] = bod\n",
        "\n",
        "            prefix = 'Subject:'\n",
        "            sub = ''\n",
        "            for line in headers.split('\\n'):\n",
        "                if line.startswith(prefix):\n",
        "                    sub = line[len(prefix):]\n",
        "                    break\n",
        "            features['subject'][i] = 'ðŸ¤”'\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # Extract the subject & body\n",
        "    ('subjectbody', SubjectBodyExtractor()),\n",
        "\n",
        "    # Use FeatureUnion to combine the features from subject and body\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list=[\n",
        "\n",
        "            # Pipeline for pulling features from the post's subject line\n",
        "            ('subject', Pipeline([\n",
        "                ('selector', ItemSelector(key='subject')),\n",
        "                ('tfidf', HashingVectorizer()),\n",
        "            ])),\n",
        "\n",
        "            # Pipeline for standard bag-of-words model for body\n",
        "            ('body_bow', Pipeline([\n",
        "                ('selector', ItemSelector(key='body')),\n",
        "                ('tfidf', TfidfVectorizer()),\n",
        "                ('best', TruncatedSVD(n_components=50)),\n",
        "            ])),\n",
        "\n",
        "            # Pipeline for pulling ad hoc features from post's body\n",
        "            ('body_stats', Pipeline([\n",
        "                ('selector', ItemSelector(key='body')),\n",
        "                ('stats', TextStats()),  # returns a list of dicts\n",
        "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "            ])),\n",
        "\n",
        "        ],\n",
        "\n",
        "        # weight components in FeatureUnion\n",
        "        transformer_weights={\n",
        "            'subject': 0.8,\n",
        "            'body_bow': 0.5,\n",
        "            'body_stats': 1.0,\n",
        "        },\n",
        "    )),\n",
        "\n",
        "    # Use a SVC classifier on the combined features\n",
        "    ('svc', SVC(kernel='linear')),\n",
        "])\n",
        "\n",
        "# limit the list of categories to make running this example faster.\n",
        "categories = ['alt.atheism', 'talk.religion.misc']\n",
        "train = fetch_20newsgroups(random_state=1,\n",
        "                           subset='train',\n",
        "                           categories=categories,\n",
        "                           )\n",
        "test = fetch_20newsgroups(random_state=1,\n",
        "                          subset='test',\n",
        "                          categories=categories,\n",
        "                          )\n",
        "\n",
        "pipeline.fit(train.data, train.target)\n",
        "y = pipeline.predict(test.data)\n",
        "print(classification_report(y, test.target))"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false,
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('venv': virtualenv)",
      "name": "python37564bitvenvvirtualenv26ba14bf3a3740c68c67968c029fd078",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.7.5-final",
      "pygments_lexer": "ipython2",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  }
}